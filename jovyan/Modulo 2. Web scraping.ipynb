{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2><font color=\"#004D7F\" size=5>Módulo 2: Recolección, preparación y almacenamiento de datos</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=6>Web scraping</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Antonio Jesús Gil</font><br>\n",
    "<font color=\"#004D7F\" size=3>Introducción a la Ciencia de Datos</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Páginas web](#section2)\n",
    "* [3. HTML](#section3)\n",
    "* [4. Parseando HTML con BeautifulSoup](#section4)\n",
    "* [5. La librería requests](#section5)\n",
    "* [6. Web scraping de una página de predicción meteorológica](#section6)\n",
    "* [7. Web scraping con un navegador real](#section7)\n",
    "* [Referencias](#referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\"> 1. Introducción</font>\n",
    "\n",
    "Cuando trabajamos como científico de datos, es común querer usar datos encontrados en Internet. En muchas ocasiones, seremos capaces de acceder a los datos en CSV o a través de un Application Programming Interface (API). Sin embargo, hay veces que los datos que queremos utilizar sólo están disponibles a través de una página web. En estos casos, podemos utilizar una técnica llamada **web scraping** para **transformar estos datos que obtenemos desde un servidor web en un formato que podamos utilizar en nuestros análisis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\"> 2. Páginas web</font>\n",
    "\n",
    "Cuando visitamos una página web, nuestro navegador hace una petición a un servidor web. Esta petición se llama una petición GET, ya que estamos obteniendo ficheros del servidor. El servidor envía de vuelta ficheros que le dicen al navegador como debe renderizar la paǵina. Estas filas pueden ser de tres tipos:\n",
    "\n",
    "* HTML: Contienen en contenido principal de la página\n",
    "* CSS: Añaden estilos a la página\n",
    "* JS: Añaden interactividad a la página — Javascript files add interactivity to web pages.\n",
    "* Contenido multimedia: Permiten a las páginas mostrar imágenes, videos, etc.\n",
    "\n",
    "Cuando nuestro navegador recive todos estos ficheros, renderiza la página y nos la muestra. Para esto, un monton de cosas suceden en segundo plano, pero no tenemos que preocuparnos de estos cuando hablamos de web scraping. Cuando hacemos web scraping, estamos interesados sólo en el contenido principal de la página, el HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section3\"></a> \n",
    "# <font color=\"#004D7F\">3. HTML</font>\n",
    "\n",
    "HyperText Markup Language (HTML) es el lenguaje con el que se crean las páginas web. HTML no es un lenguaje de programación, es un lenguaje de marcado que le dice al navegador como tiene que renderizar el contenido.\n",
    "\n",
    "A continuación se muestra una pequeña guía sobre HTML con la información mínima para poder hacer scraping de forma efectiva. HTML consiste en elementos llamados **etiquetas**. La etiqueta más básica es la etiqueta &lt;html&gt;. Esta etiqueta le dice al navegador que todo lo que encontremos dentro es HTML. Podemos crear un documento HTML mínimo usando sólamente esta etiqueta\n",
    "\n",
    "```html\n",
    "<html>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Como no hemos añadido ningún contenido a nuestra página aún, si la visualizasemos a través de un servidor web, no veríamos nada todavía.\n",
    "\n",
    "Dentro de la etiqueta html, ponemos 2 etiquetas más, las etiquetas head y body. El contenido principal de la página se encuentra dentro de la etiqueta body. La etiqueta head contiene el título de la página y otra información que en general no es útil para el web scraping.\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Como se muestra en el código anterior, las  etiquetas head y body están dentro del tag html. En HTML las etiquetas se anidan y unas etiquetas pueden colocarse dentro de otras.\n",
    "\n",
    "Ahora vamos a añadir nuestro primer contenido a la página, en forma de una etiqueta p. La etiqueta p define un parrafo y el texto dentro de la etiqueta se renderizaría como un párrafo en la página.\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p>\n",
    "            Convierte en un científico de datos.\n",
    "        </p>\n",
    "        <p>\n",
    "            Domina el lenguaje de programación que tiene más crecimiento.\n",
    "        </p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "En general usamos los siguientes nombres para referirnos a la posición de una etiqueta en relación a otras etiquetas.\n",
    "* hija: Una etiqueta hija es una etiqueta que se encuentra dentro de otra etiqueta. En el ejemplo anterior, las 2 etiquetas p son hijas de la etiqueta body.\n",
    "* padre: Un padre es una etiqueta que tiene otra dentro. En el ejemplo, html es padre de la etiqueta body.\n",
    "* hermanos: Un hermano es una etiqueta que está incluida dentro del mismo padre que otra etiqueta. Por ejemplo, head y body son hermanas, ya que ambas se encuentran dentro de la etiqueta html. \n",
    "\n",
    "También podemos agregar **propiedades** a las etiquetas HTML que cambiar su significado.\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p>\n",
    "            Convierte en un científico de datos.\n",
    "            <a href=\"https://www.etics.es\">Etic</a>\n",
    "        </p>\n",
    "        <p>\n",
    "            Domina el lenguaje de programación que tiene más crecimiento.\n",
    "            <a href=\"https://www.python.org\">Python</a>\n",
    "        </p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "En el ejemplo anteior, hemos añadido dos etiquetas a. Las etiquetas a son links y le dicen al navegador que renderice un enlace a otra página. La propiedad href de la etiqueta le dice a donde debe dirigir el enlace.\n",
    "\n",
    "Otras etiquetas comunes en HTML son las siguientes:\n",
    "\n",
    "* div: Indica una división o área de la página.\n",
    "* h1, h2, h3, h4, h5: Encabezados de distinto tamaño/importancia.\n",
    "* table: Crea una tabla\n",
    "* form: Crea un formulario de entrada\n",
    "* img: Una imagen incrustada en el documento\n",
    "\n",
    "Antes de empezar con el web scraping, es importante ver las propiedas class y id. Estas propiedas especiales le dan a los elementos HTML nombres, y hacen más fácil interactuar con el contenido sobre el que queremos hacer scraping. **Un elemento puede tener múltiples clases, y una clase puede ser compartida por varios elementos**. Cada elemento puede tener una sóla id, y esa id sólo puede ser usada una vez en la página. Las propiedades class e id son opciones, y no todos los elementos las van a tener.\n",
    "\n",
    "Podemos añadir clases a nuestro ejemplo, quedando de la siguiente forma:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"bold-paragraph\">\n",
    "            Convierte en un científico de datos.\n",
    "            <a href=\"https://www.etics.es\" id=\"course-link\">Etic</a>\n",
    "        </p>\n",
    "        <p class=\"bold-paragraph extra-large\">\n",
    "            Domina el lenguaje de programación que tiene más crecimiento.\n",
    "            <a href=\"https://www.python.org\" class=\"extra-large\">Python</a>\n",
    "        </p>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section4\"></a> \n",
    "# <font color=\"#004D7F\">4. Parseando HTML con BeautifulSoup</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para parsear documentos HTML vamos a utilizar la librería BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4\n",
    "!pip show beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar BeautifulSoup, lo primero que debemos hacer es crear un objeto de tipo BeautifulSoup. \n",
    "\n",
    "El constructor de BeautifulSoup puede aceptar dos argumentos. El primer argumento es el código HTML (o XML), y el segundo argumento es el parser que se quiere usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    " \n",
    "soup = BeautifulSoup(\"<html><head></head><body><p>Curso de ciencia de datos</p></body></html>\", \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Parsers para BeautifulSoup</font>\n",
    "\n",
    "El `html.parser` es un parser integrado en BeautifulSoup, y no funciona en versiones antiguas de Python. Otros parsers que se pueden utilizar son `lxml` y `html5lib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml==4.2.2\n",
    "#!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/linux-64::matplotlib==3.0.3=py37_1\n",
      "done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -y lxml==4.2.2\n",
    "#!conda uninstall -y lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parser `lxml` es muy rápido y es el más utilizado. Por el otro lado, el parser `html5lib` es muy lento, pero también extremadamente indulgente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><p>This is <b>invalid HTML</b></p></html>\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-05f192557870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(soup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "html = \"<html><p>This is <b>invalid HTML</p></html>\"\n",
    "# Html parser está obsoleto aunque a veces sea de utiliad\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup)\n",
    "\n",
    "# Es capaz de add etiquetas\n",
    "# Es la más rapida de las 3 y la que cumple todos los escenarios (compliance)\n",
    "#soup = BeautifulSoup(html, \"lxml\")\n",
    "#print(soup)\n",
    "\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Primeros pasos con BeautifulSoup</font>\n",
    "\n",
    "Empezaremos parseando un pequeño código HTML para ir viendo las funcionalidades de la librería.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html='''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    " <head>\n",
    "  <title>\n",
    "   ETIC: Primeros pasos con BeautifulSoup\n",
    "  </title>\n",
    " </head>\n",
    " <body>\n",
    "  <p>\n",
    "   Primer parrafo de la página\n",
    "  </p>\n",
    " </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El objeto soap contiene todo el html\n",
    "\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los distintos métodos de la librería nos podremos ir moviendo por la estrucutra del documento. Por ejemplo, podemos ir moviéndonos nivel a nivel accediendo a los hijos de una etiqueta determinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamando a los hijos del objeto soap accedemos a las etiquetas internas de html\n",
    "list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[type(item) for item in soup.children]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los items son objetos de la librería BeautifulSoup.\n",
    "\n",
    "El objeto `Doctype` contiene información acerca del tipo de documento. El objeto `Tag` contendrá en su interior el resto de etiquetas. El objeto NavigableString representa texto encontrado en el documento HTML.\n",
    "\n",
    "El objeto Tag es el más importante y nos permitirá navegar a través de todo el documento HTML y extraer otras etiquetas y texto. Podemos seleccionar el objeto `Tag` seleccinando el segundo elemento de los hijos del objeto `soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elemento 0 es Doctype\n",
    "# Elemento 1 tipo Tag\n",
    "html_tag = list(soup.children)[1]\n",
    "\n",
    "print(html_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto `html_tag` es también un objeto de tipo ' Tag', por lo que podremos también navegar a través de él."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\"> <i class=\"fa fa-book\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Demo</font>\n",
    "Vamos a ver como se podría obtener el texto dentro de la etiqueta &lt;p&gt;. Para ello navegaremos hasta la etiqueta y utilizaremos el método `get_text()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navegaremos por los children hasta llegar a <p> y despues llamamos a get_text\n",
    "# Listamos los hijos de la variable anterior 'html_tag' y contamos hasta el <body>\n",
    "body_tag = list(html_tag.children)[3]\n",
    "\n",
    "# ahora extraemos los hijos del body tag y contamos hasta llegar a <p> 0 es \\n y 1 <p>\n",
    "parrafo_tag = list(body_tag.children)[1]\n",
    "    \n",
    "# ahora obtenemos el texto y lo imprimimos por pantalla\n",
    "print(parrafo_tag.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\">Obteniendo las etiquetas de un tipo determinado</font>\n",
    "\n",
    "Lo que acabamos de ver es útil para ver como navegar por una página, pero implica varios comandos para hacer algo realmente simple. Cuando queremos extraer una etiqueta, podemos utilizar el método `find` para obtener la primera etiqueta de un tipo determinado o `find_all` para obtener la lista completa de etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\">Buscando por clase e id</font>\n",
    "\n",
    "Las propiedades class e id fueron introducidas antes. Clases y la id son propiedades que se utilizan para aplicar estilos CSS a determinados elementos HTML. Estas propiedades también se pueden utilizar para especificar sobre qué elementos queremos hacer scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html='''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>A simple example page</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>\n",
    "            <p class=\"inner-text first-item\" id=\"first\">\n",
    "                First paragraph.\n",
    "            </p>\n",
    "            <p class=\"inner-text\">\n",
    "                Second paragraph.\n",
    "            </p>\n",
    "        </div>\n",
    "        <p class=\"outer-text first-item\" id=\"second\">\n",
    "            <b>\n",
    "                First outer paragraph.\n",
    "            </b>\n",
    "        </p>\n",
    "        <p class=\"outer-text\" id=\"third\">\n",
    "            <b>\n",
    "                Third outer paragraph.\n",
    "            </b>\n",
    "        </p>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "print('# Clase inner-text')\n",
    "print(soup.find_all(class_='inner-text'))\n",
    "\n",
    "print('\\n# id: second')\n",
    "print(soup.find(id='second'))\n",
    "\n",
    "#print('\\n# id: third')\n",
    "#print(soup.find(id='third'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\">Selectores CSS</font>\n",
    "\n",
    "También podemos buscar items usando selectores CSS. Los selectores CSS es la forma en que los desarrolladores web especifican a qué elementos quieren aplicarle estilos CSS. Algunos ejemplos son los siguientes:\n",
    "\n",
    "* p a: Busca todas las etiquetas a dentro de una etiqueta p\n",
    "* body p a: Busca todas las etiquetas a dentro de una etiqueta p dentro de la etiqueta body\n",
    "* html body: Busca todas las etiqueta body dentro del tag html\n",
    "* p.outer-text: Busca todas las etiqueta p con la clase outer-text\n",
    "* p#first: Busca todas las etiquetas p con el id 'first'\n",
    "* body p.outer-text: Busca todas las etiquetas p con la clase outer-text dentro de la etiqueta body\n",
    "\n",
    "Una buena referencia sobre selectores CSS es la siguiente: [CSS Selectores en Mozilla developers](https:/Selectores en Mozilla developers/developer.mozilla.org/en-US/docs/Learn/CSS/Introduction_to_CSS/Selectors)\n",
    "You can learn more about CSS selectors here.\n",
    "\n",
    "En BeautifulSoup se pueden buscar elementos con selectores CSS utilizando el metodo `.select('selector')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('p.outer-text')\n",
    "#soup.select('p#third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Obtener todos los colaboradores del proyecto CIDAEN\n",
    "\n",
    "# Pasos\n",
    "# Ver la estructura de la pagina\n",
    "# 1 mediante las herramientas para desarrolladores del navegador buscamos los div class que se aproximen\n",
    "# para ello ir navegando por los divs hasta encontrar el child <p>\n",
    "\n",
    "# Ver en el ejemplo posterior, libreria requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> [<i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\">](#indice)</i></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section5\"></a> \n",
    "# <font color=\"#004D7F\">5. La librería requests</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que necesitamos para hacer web scraping es descargar la página. Para ello utilizaremos la libreria requests de Python. La librería hará una petición GET al servidor web, que descargará el HTML.\n",
    "\n",
    "Además de las llamadas GET, hay otros tipos de llamadas que veremos en el siguiente tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"http://www.etics.es\")\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con `page.status_code` obtendremos el código de respuesta de la solicitud. Un código de respuesta 200 significa que todo ha funcionado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder al contenido de la respuesta, el código HTML, con `page.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Combiando requests con BeautifulSoup</font>\n",
    "\n",
    "A continuación se muestra un ejemplo de cómo se podría recoger un HTML con requests y parsearlo con BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('http://www.cidaen.es')\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "for item in soup.select('div.colaborator h5'):\n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section6\"></a> \n",
    "# <font color=\"#004D7F\">6. Web scraping de una página de predicción meteorológica</font>\n",
    "\n",
    "### <font color=\"#004D7F\"> <i class=\"fa fa-book\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Demo</font>\n",
    "Vamos a ver como se podría obtener la temperatura máxima y mínima de hoy en la página [eltiempo.es](http://www.eltiempo.es).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 1 descargamos la pagina donde hariamos el web scraping\n",
    "page = requests.get('http://www.eltiempo.es/barcelona.html')\n",
    "# 2 crear el soup y el parser\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "# 3 Mirar la longitud para segurarnos si hay más divs en la pagina con esa clase\n",
    "# Si el resultado es 1 entonces es que no hay más divs y nos quedamos con este\n",
    "# div = soup.find_all('div', class_='m_table_weather_day_wrapper')\n",
    "# print(len(div))\n",
    "div = soup.find('div', class_='m_table_weather_day_wrapper')\n",
    "#print(div)\n",
    "\n",
    "# 4 Seguimos mirando la estructura de la pagina y ver que hay dentro de los divs hasta encontrar la clase que nos \n",
    "# interesa y hacemos una busqueda for\n",
    "for element in div.find_all('div', class_='m_table_weather_day_max_min'):\n",
    "    span_max = element.find('span', class_='m_table_weather_day_max_temp')\n",
    "    span_min = element.find('span', class_='m_table_weather_day_min_temp')\n",
    "    #print(span_min.get_text())\n",
    "\n",
    "# 5 Ya disponemos del div que contiene las temperaturas y vemos que yo disponen de ningun otro div, sino de span class\n",
    "# Asi que podemos utilizar la clase span dentro del bucle for\n",
    "\n",
    "# Nota extra: Si quisieramos sacar el texto a un objeto json\n",
    "\n",
    "    print({\n",
    "        \"max\": span_max.get_text(),\n",
    "        \"min\": span_min.get_text()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> [<i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\">](#indice)</i></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"section7\"></a> \n",
    "# <font color=\"#004D7F\">7. Web scraping con un navegador real</font>\n",
    "\n",
    "En algunas ocasiones es posible que no podamos obtener los datos que estamos buscando a través de la librería requests. El principal motivo para que esto ocurra es que el contenido que queremos obtener se renderiza a través de javascript y no podemos obtenerlo solamente cogiendo el código HTML de la página.\n",
    "\n",
    "Para poder hacer web scraping en páginas renderizadas por javascript necesitamos obtener la página completa (HTML, JS, CSS...), renderizarla en un navegador 'real' y obtener el código HTML resultante después de la ejecución del código javascript.\n",
    "\n",
    "**Selenium** es una librería en Python que nos permitirá interactuar con un navegador real que tengamos instalado en nuestra máquina (Chrome, Firefox...) o también usar un navegador 'headless' específico. En esta práctica utilizaremos el navegador **phantomJS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "def get_from_phantomJS(url):\n",
    "    driver = webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "    text = driver.page_source\n",
    "    driver.quit()\n",
    "    return text\n",
    "\n",
    "print(get_from_phantomJS('http://www.cidaen.es'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio</font>\n",
    "\n",
    "Obtener los datos de demanda de energía eléctrica de la página [https://demanda.ree.es/movil/peninsula/demanda/tablas/1](https://demanda.ree.es/movil/peninsula/demanda/tablas/1).\n",
    "\n",
    "Para cada fila de la tabla, se debe obtener la fecha/hora y las demandas real, estimada y programada. El resultado final debe ser una lista de diccionarios python. Cada diccionario contendrá las claves `fecha`, `real`, `estimada` y `programada`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta pagina se debe hacer con un navegador headless ya que no dispone de codigo html\n",
    "# Se renderiza completamente mediante javascript por tanto con requests no obtendriamos nada\n",
    "# Ahora nos quedamos con los div que tienen la fecha, real, estimada y programada\n",
    "\n",
    "from selenium import webdriver\n",
    "def get_from_phantomJS(url):\n",
    "    driver = webdriver.PhantomJS()\n",
    "    driver.get(url)\n",
    "    text = driver.page_source\n",
    "    driver.quit()\n",
    "    return text\n",
    "\n",
    "# Add la pagina a una variable\n",
    "pagina = get_from_phantomJS('https://demanda.ree.es/movil/peninsula/demanda/tablas/1')\n",
    "\n",
    "# print(get_from_phantomJS('https://demanda.ree.es/movil/peninsula/demanda/tablas/1'))\n",
    "\n",
    "# Crear objeto soup \n",
    "soup = BeautifulSoup(pagina, 'lxml')\n",
    "# Diccionario\n",
    "data = []\n",
    "\n",
    "# Ahora nos vamos a herramientas para desarrolladores y navegamos hasta encontrar la tabla que contiene los datos\n",
    "# Procedimiento: \n",
    "# 1 imprimimos el body (tbody) y buscamos los try td. usamos un bucle for\n",
    "tbody = soup.find('tbody')\n",
    "for tr in tbody.find_all('tr'):\n",
    "    # print(tr)\n",
    "    # 2 Interesa la informacion que hay dentro de los td\n",
    "    tds = tr.select('td')\n",
    "    # print(tds) \n",
    "    # Vemos que el primer elemento de la lista devuelto es vacio ya que corresponde a la cabecera y contiene 'th'\n",
    "    # Los siguientes 4 td son los buenos\n",
    "    if tds:                   # En python un if de lista vacia evalua a false y no devuelve valor de la primera fila\n",
    "        element = {\n",
    "            'fecha': tds[0].get_text(),\n",
    "            'real': tds[1].get_text(),\n",
    "            'estimada' : tds[2].get_text(),\n",
    "            'programada': tds[3].get_text()\n",
    "        }\n",
    "        data.append(element)\n",
    "print(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"referencias\"></a>\n",
    "# <font color=\"#004D7F\"> Referencias</font>\n",
    "\n",
    "* [requests](http://docs.python-requests.org/en/master/)\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [selenium](http://www.seleniumhq.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> [<i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\">](#indice)</i></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
